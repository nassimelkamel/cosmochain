# -*- coding: utf-8 -*-
"""classification_nassim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DxJAZLThLGrh2MVhkrDuPWbgwJFxZnM

#Regression :  Predire le prix unitaire d'un produit (y) selon les materieles , la quantitÃ© des materielles , la categorie des materielles  .
"""

import pandas as pd

# 1. Lire les fichiers Excel (remplace les noms des fichiers par les tiens)
fact_procurement = pd.read_csv("Fact_Procurement.csv")
dim_products = pd.read_csv("Dim_Products.csv")
dim_date = pd.read_csv("Dim_Date.csv")
dim_materials = pd.read_csv("Dim_Materials.csv")

# 2. Jointures
merged_df = fact_procurement.merge(dim_products, left_on="Product_FK", right_on="Product_PK", how="inner") \
                            .merge(dim_date, left_on="Date_FK", right_on="Date_PK", how="inner")\
                            .merge(dim_materials, left_on="Material_FK", right_on="Material_PK", how="inner")

# 3. Filtrage
final_df = merged_df[(merged_df["prix"].notna()) & (merged_df["prix"] > 0)]

# 4. SÃ©lection des colonnes souhaitÃ©es
df = final_df[[
     "Unit_Price",
     "Product_Name",
     "Quantite",
     "Material_Category"
 ]]
# (Optionnel) Affichage ou sauvegarde
print(df.head())
# final_df.to_csv("result.csv", index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pickle

# SÃ©paration des features et de la cible
X = df[['Product_Name', 'Quantite', 'Material_Category']]
y = df['Unit_Price']

# Encodage des variables catÃ©gorielles
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
X_encoded = encoder.fit_transform(X[['Product_Name', 'Material_Category']])

# ConcatÃ©nation avec la variable quantitative
import numpy as np
X_final = np.concatenate([X_encoded, X[['Quantite']].values], axis=1)

# Split des donnÃ©es
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# EntraÃ®nement du modÃ¨le
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# PrÃ©dictions
y_pred = model.predict(X_test)

# Ã‰valuation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R2 Score: {r2:.2f}")

dim_materials

"""#Cluster

material expiry risk . expiredate , manufactor date  , quantity used ( procurement )

"""

import pandas as pd

# 1. Lire les fichiers CSV
clust_df = pd.read_csv("Fact_Storage.csv")
dim_products = pd.read_csv("Dim_Products.csv")
dim_date = pd.read_csv("Dim_Date.csv")

# 2. Renommer les colonnes de dim_date pour Ã©viter les conflits
dim_date_manuf = dim_date.rename(columns={
    "Date_PK": "Manufacture_Date_FK",
    "FullDate": "Manufacture_Date"
})

dim_date_exp = dim_date.rename(columns={
    "Date_PK": "Expiration_date_FK",
    "FullDate": "Expiration_Date"
})

# 3. Fusionner les donnÃ©es
df = clust_df.merge(dim_products, left_on="product_FK", right_on="Product_PK", how="inner") \
                    .merge(dim_date_manuf, on="Manufacture_Date_FK", how="left") \
                    .merge(dim_date_exp, on="Expiration_date_FK", how="left")

# (Optionnel) Afficher un aperÃ§u


data = df[[
    "rest_quantity",
    "Expiration_Date",
    "Manufacture_Date"

]]

print(data.head())

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
import joblib

# â³ Conversion des dates en jours
data['Expiration_Date'] = pd.to_datetime(data['Expiration_Date'])
data['Manufacture_Date'] = pd.to_datetime(data['Manufacture_Date'])

# CrÃ©er des nouvelles variables numÃ©riques
data['shelf_life'] = (data['Expiration_Date'] - data['Manufacture_Date']).dt.days
data['days_since_manufacture'] = (pd.Timestamp.today() - data['Manufacture_Date']).dt.days

# SÃ©lection des features
features = data[['rest_quantity', 'shelf_life', 'days_since_manufacture']]

# ðŸ”„ Normalisation
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# ðŸ“Š KMeans Clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(features_scaled)

# Ajout des clusters au DataFrame
data['cluster'] = clusters

# ðŸ“ˆ Ã‰valuation
sil_score = silhouette_score(features_scaled, clusters)
db_index = davies_bouldin_score(features_scaled, clusters)

print("Silhouette Score :", sil_score)
print("Davies-Bouldin Index :", db_index)

# ðŸ’¾ Export du modÃ¨le et du scaler
joblib.dump(kmeans, 'kmeans_model.pkl')
joblib.dump(scaler, 'kmeans_scaler.pkl')

import matplotlib.pyplot as plt
import seaborn as sns

# Configuration des graphes
plt.figure(figsize=(15, 4))

# ðŸ”¹ Graphe 1 : rest_quantity
plt.subplot(1, 3, 1)
sns.boxplot(data=data, x='cluster', y='rest_quantity', palette='Set2')
plt.title('rest_quantity par cluster')

# ðŸ”¹ Graphe 2 : shelf_life
plt.subplot(1, 3, 2)
sns.boxplot(data=data, x='cluster', y='shelf_life', palette='Set2')
plt.title('shelf_life par cluster')

# ðŸ”¹ Graphe 3 : days_since_manufacture
plt.subplot(1, 3, 3)
sns.boxplot(data=data, x='cluster', y='days_since_manufacture', palette='Set2')
plt.title('days_since_manufacture par cluster')

plt.tight_layout()
plt.show()

"""#Serie Temprolle"""

!pip install prophet

import pandas as pd
import matplotlib.pyplot as plt

# 1. Lire les fichiers Excel
fact_sales = pd.read_csv("Fact_Sales.csv")
dim_products = pd.read_csv("Dim_Products.csv")
dim_date = pd.read_csv("Dim_Date.csv")

merged_df = fact_sales.merge(dim_products, left_on="PRODUCT_FK", right_on="Product_PK", how="inner") \
                      .merge(dim_date, left_on="DATE_FK", right_on="Date_PK", how="inner")


merged_df["x"] = merged_df["Unit_Price"] * merged_df["Quantity"]


serie_df = merged_df.groupby("FullDate")["x"].sum().reset_index()


serie_df.rename(columns={"FullDate": "t"}, inplace=True)

print(serie_df.head(10))


serie_df["t"] = pd.to_datetime(serie_df["t"])

# 2. Tri chronologique
serie_df = serie_df.sort_values("t")

# 3. DÃ©finir la date comme index (utile pour les sÃ©ries temporelles)
serie_df.set_index("t", inplace=True)


# 3. Regrouper par mois avec somme
monthly_df = serie_df.resample("M").sum()  # "M" = fin de chaque mois

# 4. Tracer la sÃ©rie temporelle mensuelle
plt.figure(figsize=(10, 5))
plt.plot(monthly_df.index, monthly_df["x"], marker='o', linestyle='-')
plt.title("Ventes mensuelles (x = prix * quantitÃ©)")
plt.xlabel("Mois")
plt.ylabel("Total mensuel (x)")
plt.grid(True)
plt.tight_layout()
plt.show()

from prophet import Prophet

df_prophet = monthly_df.reset_index().rename(columns={"t": "ds", "x": "y"})

# 3. Initialiser et entraÃ®ner le modÃ¨le
model_st = Prophet()
model_st.fit(df_prophet)

future = model_st.make_future_dataframe(periods=6, freq='M')

# 5. Faire la prÃ©diction
forecast = model_st.predict(future)

# 6. Visualiser la prÃ©vision
fig1 = model_st.plot(forecast)
plt.title("PrÃ©vision des ventes mensuelles")
plt.xlabel("Date")
plt.ylabel("x (Prix * QuantitÃ©)")
plt.grid(True)
plt.show()

fig2 = model_st.plot_components(forecast)
plt.show()

predictions = forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]]

print(predictions.tail(6))  # Show only the future predictions

import joblib

joblib.dump(model_st, "St_model.pkl")

print("âœ… Le modÃ¨le a Ã©tÃ© exportÃ© sous 'St_model.pkl'")

"""#classification"""

import pandas as pd

# 1. Lire les fichiers CSV
clust_df = pd.read_csv("Fact_Storage.csv")
dim_products = pd.read_csv("Dim_Products.csv")
dim_date = pd.read_csv("Dim_Date.csv")

# 2. Renommer les colonnes de dim_date pour Ã©viter les conflits
dim_date_manuf = dim_date.rename(columns={
    "Date_PK": "Manufacture_Date_FK",
    "FullDate": "Manufacture_Date"
})

dim_date_exp = dim_date.rename(columns={
    "Date_PK": "Expiration_date_FK",
    "FullDate": "Expiration_Date"
})

# 3. Fusionner les donnÃ©es
df = clust_df.merge(dim_products, left_on="product_FK", right_on="Product_PK", how="inner") \
                    .merge(dim_date_manuf, on="Manufacture_Date_FK", how="left") \
                    .merge(dim_date_exp, on="Expiration_date_FK", how="left")

df['Expiration_Date'] = pd.to_datetime(df['Expiration_Date'])
df['Manufacture_Date'] = pd.to_datetime(df['Manufacture_Date'])

# Creating new features based on existing columns in 'df'
df['shelf_life'] = (df['Expiration_Date'] - df['Manufacture_Date']).dt.days
df['days_since_manufacture'] = (pd.Timestamp.today() - df['Manufacture_Date']).dt.days

df = df[[
    "rest_quantity",
    "Manufacture_Date",
    "Expiration_Date",
    "Unit_Price",
    "Quantite",
    "rest_quantity"

]]
dfinfo = df.info()
print(dfinfo)

import pandas as pd

# Conversion des dates
df["Manufacture_Date"] = pd.to_datetime(df["Manufacture_Date"])
df["Expiration_Date"] = pd.to_datetime(df["Expiration_Date"])

# Calcul des durÃ©es
df["shelf_life"] = (df["Expiration_Date"] - df["Manufacture_Date"]).dt.days
df["days_since_manufacture"] = (pd.Timestamp.today() - df["Manufacture_Date"]).dt.days

# Fonction de classification
def classify(row):
    # SÃ©curitÃ© : Ã©viter la division par zÃ©ro
    if row["shelf_life"] == 0:
        return "Classe 2"

    time_ratio = row["days_since_manufacture"] / row["shelf_life"]
    # Utiliser uniquement des scalaires ici
    if (
        time_ratio > 0.4 and
        row["Unit_Price"] > 80
    ):
        return "Classe 1"
    else:
        return "Classe 2"

# Application ligne par ligne
df["classe"] = df.apply(classify, axis=1)
import pandas as pd

# Conversion des dates
df["Manufacture_Date"] = pd.to_datetime(df["Manufacture_Date"])
df["Expiration_Date"] = pd.to_datetime(df["Expiration_Date"])

# Calcul des durÃ©es
df["shelf_life"] = (df["Expiration_Date"] - df["Manufacture_Date"]).dt.days
df["days_since_manufacture"] = (pd.Timestamp.today() - df["Manufacture_Date"]).dt.days

# Fonction de classification
def classify(row):
    # SÃ©curitÃ© : Ã©viter la division par zÃ©ro
    if row["shelf_life"] == 0:
        return "Classe 2"

    time_ratio = row["days_since_manufacture"] / row["shelf_life"]
    # Utiliser uniquement des scalaires ici
    if (
        time_ratio > 0.4 and
        row["Unit_Price"] > 80
    ):
        return "Classe 1"
    else:
        return "Classe 2"

# Application ligne par ligne
df["classe"] = df.apply(classify, axis=1)

# Affichage final
df=df[["rest_quantity", "Quantite", "Unit_Price", "Manufacture_Date", "Expiration_Date", "classe"]]

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler # Import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import joblib
import seaborn as sns
import matplotlib.pyplot as plt

# PrÃ©paration des donnÃ©es
X = df[["rest_quantity", "Quantite", "Unit_Price", "Manufacture_Date", "Expiration_Date"]]

# --- Convert datetime features to numerical features ---
X['Manufacture_Date'] = pd.to_numeric(pd.to_datetime(X['Manufacture_Date']))
X['Expiration_Date'] = pd.to_numeric(pd.to_datetime(X['Expiration_Date']))
# -------------------------------------------------------

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df["classe"])

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Scale numerical features using StandardScaler ---
scalerclass = StandardScaler()
X_train = scalerclass.fit_transform(X_train)
X_test = scalerclass.transform(X_test)
# ---------------------------------------------------


# EntraÃ®nement du modÃ¨le
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# PrÃ©dictions
y_pred = model.predict(X_test)

# Ã‰valuation
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

print("âœ… Accuracy:", round(accuracy_score(y_test, y_pred), 2))
print("âœ… Precision:", round(precision_score(y_test, y_pred, average='weighted'), 2))
print("âœ… Recall:", round(recall_score(y_test, y_pred, average='weighted'), 2))
print("âœ… F1-score:", round(f1_score(y_test, y_pred, average='weighted'), 2))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("PrÃ©dits")
plt.ylabel("RÃ©els")
plt.title("ðŸŽ¯ Matrice de confusion")
plt.show()

# Export du modÃ¨le
joblib.dump(model, "modele_classification_produit.pkl")
joblib.dump(scalerclass, 'class_scaler.pkl')

print("ðŸ’¾ ModÃ¨le sauvegardÃ© dans 'modele_classification_produit.pkl'")